{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crush Rig - Classifier\n",
    "Written by Matt MacDonald for CIGITI at the Hospital for Sick Children Toronto\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tools to manipulate data will be obtained from the crush_plot.py file. The objective of this notebook is to predict the histological targets from the force/position crush data using a classifier, either logistic regression or xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# or notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crush_read import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crush data must be collected using the crush rig and crush.py and stored in the expected folder structure at the root directory indicated by PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = Path('')\n",
    "# Default in crush_plot.py\n",
    "PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all data and modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = study_outline(PATH)\n",
    "targets = study_targets(PATH)\n",
    "crushes = study_data(study)\n",
    "crushes = modify(crushes)\n",
    "crushes = calculate(crushes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for regression and xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, legend = prep(crushes, targets)\n",
    "y = refine(y)\n",
    "print('Reference for categorical features:')\n",
    "legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in y.columns:\n",
    "    s = y[col].sum()\n",
    "    c = y[col].count()\n",
    "    r = s / c\n",
    "    print(f\"{col}: {s}/{c} ({r * 100:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major tissue damage target is unbalanced. It may not be enough data for an accurate classifier.\n",
    "\n",
    "Generate matrix of correlations to aid understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pd.concat([X, y], axis=1)\n",
    "W_corr = W.corr(method='spearman')\n",
    "sns.heatmap(W_corr, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the key variable which is target stress. Below is the corresponding load in grams for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for load in np.arange(0, 1300, 100):\n",
    "    stress = (9.81 * load / 1000) / (np.pi * (5/2)** 2)\n",
    "    print(f\"{stress:5.2} (MPa) = {load:5} (grams)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = 'Target Stress (MPa)'\n",
    "for y_name in y.columns:\n",
    "    plt.figure()\n",
    "    plt.scatter(x=X[x_name], y=y[y_name])\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any histology related features to focus on real time predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X.copy()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('Pathologist (Cathy or Corwyn)', axis=1)\n",
    "X = X.drop('Serosal Thickness (mm)', axis=1)\n",
    "X = X.drop('Post Serosal Thickness (mm)', axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_metrics(cm, disp=False):\n",
    "    acc = 100 * (cm[0][0] + cm[1][1]) / cm.sum()\n",
    "    f_pos = 100 * cm[0][1] / cm.sum()\n",
    "    f_neg = 100 * cm[1][0] / cm.sum()\n",
    "    \n",
    "    if disp:\n",
    "        print(f'Accuracy = {acc:.2f}%')\n",
    "        print(f'False Positives = {f_pos:.2f}%')\n",
    "        print(f'False Negatives = {f_neg:.2f}%')\n",
    "    else:\n",
    "        return acc, f_pos, f_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_predict(model, dataset, disp=True):\n",
    "    # Predict and make a confusion matrix, optionally display results\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    clf = model[0]\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return y_pred, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_model(X, y, seed=0, size=0.25):\n",
    "    # Convert from pandas to numpy\n",
    "    X_np = X.values.astype(np.float64)\n",
    "    y_np = y.values.ravel()\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=size, random_state=seed)\n",
    "\n",
    "    # Scale input features\n",
    "    scl = StandardScaler()\n",
    "    X_train = scl.fit_transform(X_train)\n",
    "    X_test = scl.transform(X_test)\n",
    "\n",
    "    # Fit logistic regression to training set\n",
    "    clf = LogisticRegression(random_state=seed,\n",
    "                             solver='lbfgs',\n",
    "                             max_iter=1000,\n",
    "                             n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return (clf, scl), (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_features(X, y, n_features, seed=0):\n",
    "    X_np = X.values.astype(np.float64)\n",
    "    y_np = y.values.ravel()\n",
    "    \n",
    "    clf = LogisticRegression(random_state=seed,\n",
    "                             solver='lbfgs',\n",
    "                             max_iter=1000,\n",
    "                             n_jobs=4)\n",
    "    rfe = RFE(clf, n_features)  # pick the best features\n",
    "    rfe = rfe.fit(X_np, y_np)\n",
    "    return rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_bndr(model, dataset, n1):\n",
    "    # Plot a series of 2D decision boundaries freezing the other features to mean values\n",
    "    # dataset=(X, y) ideally of the test set and n1 is the primary feature being compared (default stress)\n",
    "    X_set, y_set = dataset[0], dataset[1]\n",
    "    clf = model[0]\n",
    "    X_avg = np.mean(X_set, axis=0).reshape(1, -1)\n",
    "    \n",
    "    def clf_grid(grid1, grid2, n1, n2):\n",
    "        # Predict for two 2D grids of features\n",
    "        s = grid1.shape\n",
    "        Z = np.zeros(s)\n",
    "        for i in range(s[0]):\n",
    "            for j in range(s[1]):\n",
    "                X = X_avg\n",
    "                X[0, n1] = grid1[i, j]\n",
    "                X[0, n2] = grid2[i, j]\n",
    "                Z[i, j] = clf.predict(X)\n",
    "        return Z\n",
    "\n",
    "    # Visualize the decision boundary\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    colors = ('blue', 'red')\n",
    "    for n2 in range(len(X.columns)):\n",
    "        if n2 == n1:\n",
    "            continue\n",
    "        X1, X2 = np.meshgrid(np.linspace(X_set[:, n1].min() - 1, X_set[:, n1].max() + 1, 100),\n",
    "                             np.linspace(X_set[:, n2].min() - 1, X_set[:, n2].max() + 1, 100))\n",
    "        Z = clf_grid(X1, X2, n1, n2)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.contourf(X1, X2, Z, alpha=0.75, cmap=ListedColormap(colors))\n",
    "        plt.xlim(X1.min(), X1.max())\n",
    "        plt.ylim(X2.min(), X2.max())\n",
    "        for res in np.unique(y_set):\n",
    "            color = colors[0] if (res == False) else colors[1]\n",
    "            plt.scatter(X_set[y_set == res, n1], X_set[y_set == res, n2],\n",
    "                        c=color, label=res)\n",
    "        plt.title('Classification (N={})'.format(len(y_set)))\n",
    "        plt.xlabel(X.columns[n1])\n",
    "        plt.ylabel(X.columns[n2])\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a specific indicator from the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = 'Significant Serosal Change'\n",
    "# indicator = 'Tissue Damage'\n",
    "# indicator = 'Major Tissue Damage'\n",
    "y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any features deemed to be irrelevent by recursive feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = logreg_features(X, y[indicator], 3)\n",
    "rank = pd.DataFrame({'feature': X.columns.values,\n",
    "                     'support': rfe.support_,\n",
    "                     'ranking': rfe.ranking_})\n",
    "rank = rank.sort_values(by='ranking')\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['Tissue (COLON or SB)', 'Target Stress (MPa)', 'Target Stiffness (MPa)']]\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = pd.Series(index=range(100))\n",
    "for seed in range(100):\n",
    "    model, train, test = logreg_model(X, y[indicator], seed=seed)\n",
    "    _, cm = logreg_predict(model, test)\n",
    "    _, cm_tr = logreg_predict(model, train)\n",
    "    acc, f_pos, f_neg = logreg_metrics(cm)\n",
    "    acc_tr, f_pos_tr, f_neg_tr = logreg_metrics(cm_tr)\n",
    "    w = 3\n",
    "    accs[seed] = ((acc + acc_tr) / 2) - w * (f_neg + f_neg_tr) / 2\n",
    "\n",
    "print(f\"Average: metric={accs.mean():.2f}\")\n",
    "print(f\"Worst: seed={accs.idxmin()}, metric={accs.min():.2f}\")\n",
    "print(f\"Best: seed={accs.idxmax()}, metric={accs.max():.2f}\")\n",
    "model, train, test = logreg_model(X, y[indicator], seed=accs.idxmax())\n",
    "y_pred, cm = logreg_predict(model, test)\n",
    "logreg_metrics(cm, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics for comparison\n",
    "logreg_metrics(logreg_predict(model, train)[1], disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = X.columns.get_loc('Target Stress (MPa)')\n",
    "decision_bndr(model, train, n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_bndr(model, test, n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model[0]\n",
    "scl = model[1]\n",
    "arr = np.column_stack([(clf.coef_ / scl.scale_).T, X.columns.values])\n",
    "\n",
    "importance = pd.DataFrame(arr, columns=['weight', 'feature'])\n",
    "importance = importance.append({'weight': clf.intercept_[0], 'feature': 'Intercept'},\n",
    "                               ignore_index=True)\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.sort_values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build xgboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_corr = y_pred == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf.predict(X_train)\n",
    "y_corr_train = y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test acc = {sum(y_corr) / len(y_corr)}\")\n",
    "print(f\"train acc = {sum(y_corr_train) / len(y_corr_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "plot_tree(clf, rankdir='LR', num_trees=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature numbers legend\n",
    "for i, feat in enumerate(X.columns):\n",
    "    print(f\"f{i} = {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y], axis=1)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prot in [0, 1]:\n",
    "    prot_str = f\"Protocol[{prot}]\"\n",
    "    avg = df.loc[df['Protocol'] == prot, 'Damage Score'].mean()\n",
    "    print(f\"{legend[prot_str]} average damage = {avg}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
